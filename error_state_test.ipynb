{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2: Language modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "import math\n",
    "import queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "\n",
    "    ROOT = ['<root>', '<root>', 0]  # Pseudo-root\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.filename, 'rt', encoding='utf-8') as lines:\n",
    "            tmp = [Dataset.ROOT]\n",
    "            for line in lines:\n",
    "                if not line.startswith('#'):  # Skip lines with comments\n",
    "                    line = line.rstrip()\n",
    "                    if line:\n",
    "                        columns = line.split('\\t')\n",
    "                        if columns[0].isdigit():  # Skip range tokens\n",
    "                            tmp.append([columns[1], columns[3], int(columns[6])])\n",
    "                    else:\n",
    "                        yield tmp\n",
    "                        tmp = [Dataset.ROOT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_conllu():\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.filename, 'rt', encoding='utf-8') as lines:\n",
    "            tmp = []\n",
    "            for line in lines:\n",
    "                if not line.startswith('#'):  # Skip lines with comments\n",
    "                    line = line.rstrip()\n",
    "                    if line:\n",
    "                        columns = line.split('\\t')\n",
    "                        if columns[0].isdigit():  # Skip range tokens\n",
    "                            tmp.append(columns)\n",
    "                    else:\n",
    "                        yield tmp\n",
    "                        tmp = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "\n",
    "def make_vocabs(gold_data):\n",
    "    # TODO: Replace the next line with your own code\n",
    "    # Initializatioin\n",
    "    word_vocab = {PAD:0, UNK:1}\n",
    "    tag_vocab = {PAD:0}\n",
    "    # Go through data\n",
    "    for sentence in gold_data:\n",
    "        for word, tag, _ in sentence:\n",
    "            # new word\n",
    "            if word not in word_vocab:\n",
    "                word_vocab[word] = len(word_vocab)\n",
    "            # new tag\n",
    "            if tag not in tag_vocab:\n",
    "                tag_vocab[tag] = len(tag_vocab)\n",
    "    return word_vocab, tag_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger(object):\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed window model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_specs, hidden_dim, output_dim, word_pretrained=None):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        super().__init__()\n",
    "        # Extracting specs\n",
    "        self.window_sizes = [m for (m, n, e) in embedding_specs]\n",
    "        self.num_sources = [n for (m, n, e) in embedding_specs]\n",
    "        self.embed_dims = [e for (m, n, e) in embedding_specs]\n",
    "        \n",
    "        # Embedding layers are stored in nn module list \n",
    "        self.embeddings = nn.ModuleList() \n",
    "        # For pretrained word embeddings\n",
    "        if word_pretrained != None:\n",
    "            # word embedding\n",
    "            self.embeddings.append(nn.Embedding.from_pretrained(word_pretrained, freeze=False))\n",
    "            # tag embedding (still need training)\n",
    "            tag_num = self.num_sources[-1]\n",
    "            tag_embed_dim = self.embed_dims[-1]\n",
    "            self.embeddings.append(nn.Embedding(tag_num, tag_embed_dim))\n",
    "        # Non-pretrained\n",
    "        else:\n",
    "            for window_size, num_source, embed_dim in embedding_specs:\n",
    "                # embedding layer\n",
    "                embedding = nn.Embedding(num_source, embed_dim)\n",
    "                # Initialize weights from normal distrubution ~ N(0, 0.01)\n",
    "                embedding.weight.data.normal_(0, 1e-2)\n",
    "                # append to module list\n",
    "                self.embeddings.append(embedding)\n",
    "        \n",
    "        \n",
    "        # Linear layers\n",
    "        self.concat_len = sum([m*e for m, e in zip(self.window_sizes, self.embed_dims)])\n",
    "        self.linear_1 = nn.Linear(self.concat_len, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, features):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        # batch size\n",
    "        B = features.shape[0]\n",
    "        # Embedding and concatenation\n",
    "        f_index = 0\n",
    "        for i, (m, e) in enumerate(zip(self.window_sizes, self.embed_dims)):\n",
    "            if i == 0:\n",
    "                embedded = self.embeddings[i](features[:, f_index:f_index+m]).view((B, m*e))\n",
    "            if i > 0:\n",
    "                to_be_cat = self.embeddings[i](features[:, f_index:f_index+m]).view((B, m*e))\n",
    "                embedded = torch.cat((embedded, to_be_cat), dim=1) \n",
    "            f_index += m\n",
    "        # Feed forward\n",
    "        x = self.linear_1(embedded)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual tagger class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowTagger(Tagger):\n",
    "\n",
    "    def __init__(self, vocab_words, vocab_tags, output_dim, word_dim=50, tag_dim=10, hidden_dim=100, word_pretrained=None):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        self.window_size = 3\n",
    "        self.vocab_tags = vocab_tags\n",
    "        self.vocab_words = vocab_words\n",
    "        # Embedding specs\n",
    "        embedding_specs = [(self.window_size, len(vocab_words), word_dim), (1, len(vocab_tags), tag_dim)]\n",
    "        # Initialize model\n",
    "        if word_pretrained != None:\n",
    "            self.model = FixedWindowModel(embedding_specs, hidden_dim, output_dim, word_pretrained=word_pretrained)\n",
    "        else:\n",
    "            self.model = FixedWindowModel(embedding_specs, hidden_dim, output_dim)\n",
    "\n",
    "    def featurize(self, words, i, pred_tags):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        m = self.window_size\n",
    "        half_m = int((m-1) / 2)\n",
    "        features = []\n",
    "         \n",
    "        # Get word id in the window \n",
    "        for j in range(i - half_m, i + half_m + 1):\n",
    "            # need padding at head and tail\n",
    "            if j < 0 or j >= len(words):\n",
    "                features.append(0)\n",
    "            else:\n",
    "                features.append(words[j])\n",
    "        \n",
    "        # Append predicted tags\n",
    "        features += pred_tags\n",
    "            \n",
    "        return torch.tensor(features)\n",
    "\n",
    "    def predict(self, words):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        # Initialization\n",
    "        predictions = []\n",
    "        pred_tags = [0]\n",
    "        word_ids = []\n",
    "        # Encode words into their ids\n",
    "        for w in words:\n",
    "            try:\n",
    "                word_ids.append(self.vocab_words[w])\n",
    "            except KeyError:\n",
    "                # tag id for unknown words is '1'\n",
    "                word_ids.append(1)\n",
    "            \n",
    "        # Go through sentence\n",
    "        for i, word_id in enumerate(words):\n",
    "            # Get feature vector\n",
    "            features = self.featurize(word_ids, i , pred_tags)\n",
    "            features = features.reshape((1, 4))\n",
    "            # Feed through the network\n",
    "            output = self.model.forward(features)\n",
    "            # Get the predicted tag\n",
    "            hash_value = torch.argmax(output).item()\n",
    "            predicted_tag = list(self.vocab_tags.keys())[hash_value]\n",
    "            predictions.append(predicted_tag)\n",
    "            # update pred_tag\n",
    "            pred_tags = [hash_value]\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_accuracy(tagger, gold_data):\n",
    "    # TODO: Replace the next line with your own code\n",
    "    # Initialization\n",
    "    num_of_pairs = 0\n",
    "    correct_count = 0\n",
    "    # Go through sentences\n",
    "    for sentence in gold_data:\n",
    "        # Extract words and tags\n",
    "        words = []\n",
    "        tags = []\n",
    "        for word, tag, _ in sentence:\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "            num_of_pairs += 1\n",
    "        # predict with tagger\n",
    "        predictions = tagger.predict(words)\n",
    "        # count correct predictions\n",
    "        for prediction, label in zip(predictions, tags):\n",
    "            if prediction == label:\n",
    "                correct_count += 1\n",
    "                \n",
    "    # return accuracy\n",
    "    return correct_count / num_of_pairs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "\n",
    "    def predict(self, words, tags):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Arc Standard Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcStandardParser(Parser):\n",
    "\n",
    "    MOVES = tuple(range(3))\n",
    "\n",
    "    SH, LA, RA = MOVES  # Parser moves are specified as integers.\n",
    "\n",
    "    @staticmethod\n",
    "    def initial_config(num_words):\n",
    "        return (0, [], num_words * [0])\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_moves(config):\n",
    "        valid_moves = []\n",
    "        # Get configuration\n",
    "        i, stack, heads = config\n",
    "        # Shift is available when there's words in buffer\n",
    "        if i < len(heads):\n",
    "            valid_moves.append(ArcStandardParser.SH)\n",
    "        # Left and right arcs are available when there are at least two words in stack\n",
    "        if len(stack) >= 2:\n",
    "            valid_moves.append(ArcStandardParser.LA)\n",
    "            valid_moves.append(ArcStandardParser.RA)\n",
    "            \n",
    "        return valid_moves\n",
    "\n",
    "    @staticmethod\n",
    "    def next_config(config, move):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        # Get current configuration\n",
    "        i = config[0]\n",
    "        stack, heads = [x.copy() for x in config[1:]]\n",
    "        # Shift\n",
    "        if move == ArcStandardParser.SH:\n",
    "            # move word to stack\n",
    "            stack.append(i)\n",
    "            # update buffer\n",
    "            i += 1\n",
    "            \n",
    "        # Left arc\n",
    "        elif move == ArcStandardParser.LA:\n",
    "            # assign head \n",
    "            heads[stack[-2]] = stack[-1]\n",
    "            # remove second item in stack\n",
    "            del stack[-2]\n",
    "        # Right arc\n",
    "        elif move == ArcStandardParser.RA:\n",
    "            # assign head \n",
    "            heads[stack[-1]] = stack[-2]\n",
    "            # remove first item in stack\n",
    "            del stack[-1]\n",
    "            \n",
    "        return (i, stack, heads)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_final_config(config):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        # Get current configuration\n",
    "        i, stack, heads = config\n",
    "        # Buffer clear\n",
    "        if i == len(heads):\n",
    "            # Stack clear (only one 'root' left)\n",
    "            if len(stack) == 1:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generater that yield oracle moves during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle_moves(gold_heads):\n",
    "    # Helper function to check if the word is 'used up' as head\n",
    "    def check_used_up(i, our_heads):\n",
    "        # count the number of times when i is used as head in gold data\n",
    "        gold_count = sum([x == i for x in gold_heads])\n",
    "        # count ours\n",
    "        our_count = sum([x == i for x in our_heads])\n",
    "        return gold_count == our_count\n",
    "    # Initialization\n",
    "    SH, LA, RA = tuple(range(3))\n",
    "    parser = ArcStandardParser()\n",
    "    # Keep track of stack and our generated heads\n",
    "    stack = []\n",
    "    i = 0\n",
    "    our_heads = [0] * len(gold_heads)\n",
    "    config = (i, stack, our_heads)\n",
    "    # Not in final config\n",
    "    while not parser.is_final_config(config):\n",
    "        i, stack, our_heads = config\n",
    "        # When there's more than 2 items in stack\n",
    "        if len(stack) >= 2:\n",
    "            # Choose LA if the arc is in gold heads and all the arc from second-topmost has been assigned\n",
    "            if gold_heads[stack[-2]] == stack[-1] and check_used_up(stack[-2], our_heads):\n",
    "                yield config, LA\n",
    "                config = parser.next_config(config, LA)\n",
    "            # Choose RA\n",
    "            elif gold_heads[stack[-1]] == stack[-2] and check_used_up(stack[-1], our_heads):\n",
    "                yield config, RA\n",
    "                config = parser.next_config(config, RA)\n",
    "            # Otherwise, SH\n",
    "            else:\n",
    "                yield config, SH\n",
    "                config = parser.next_config(config, SH)\n",
    "        # Can only do SH\n",
    "        else:\n",
    "            yield config, SH\n",
    "            config = parser.next_config(config, SH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle_moves_with_error_state(gold_heads):\n",
    "    # Helper function to check if the word is 'used up' as head\n",
    "    def check_used_up(i, our_heads):\n",
    "        # count the number of times when i is used as head in gold data\n",
    "        gold_count = sum([x == i for x in gold_heads])\n",
    "        # count ours\n",
    "        our_count = sum([x == i for x in our_heads])\n",
    "        return gold_count == our_count\n",
    "    # Helper function to generate two other 'error examples'\n",
    "    def error_states(config, gold_move):\n",
    "        # check if the other two moves are valid\n",
    "        error_states = []\n",
    "        for move in parser.valid_moves(config):\n",
    "            if move != gold_move:\n",
    "                # move one step further\n",
    "                error_config = parser.next_config(config, move)\n",
    "                # yield error state\n",
    "                error_states.append((error_config, ES))\n",
    "                #except IndexError:\n",
    "                #    print('error!!!!')\n",
    "                #    #print(config, move)\n",
    "        return error_states\n",
    "    # Initialization\n",
    "    SH, LA, RA, ES = tuple(range(4))\n",
    "    parser = ArcStandardParser()\n",
    "    # Keep track of stack and our generated heads\n",
    "    stack = []\n",
    "    i = 0\n",
    "    our_heads = [0] * len(gold_heads)\n",
    "    config = (i, stack, our_heads)\n",
    "    # Not in final config\n",
    "    while not parser.is_final_config(config):\n",
    "        i, stack, our_heads = config\n",
    "        # When there's more than 2 items in stack\n",
    "        if len(stack) >= 2:\n",
    "            # Choose LA if the arc is in gold heads and all the arc from second-topmost has been assigned\n",
    "            if gold_heads[stack[-2]] == stack[-1] and check_used_up(stack[-2], our_heads):\n",
    "                yield config, LA\n",
    "                for e_config, m in error_states(config, LA):\n",
    "                    yield e_config, m \n",
    "                config = parser.next_config(config, LA)\n",
    "            # Choose RA\n",
    "            elif gold_heads[stack[-1]] == stack[-2] and check_used_up(stack[-1], our_heads):\n",
    "                yield config, RA\n",
    "                for e_config, m in error_states(config, RA):\n",
    "                    yield e_config, m \n",
    "                config = parser.next_config(config, RA)\n",
    "            # Otherwise, SH\n",
    "            else:\n",
    "                yield config, SH\n",
    "                for e_config, m in error_states(config, SH):\n",
    "                    yield e_config, m \n",
    "                config = parser.next_config(config, SH)\n",
    "        # Can only do SH\n",
    "        else:\n",
    "            yield config, SH\n",
    "            for e_config, m in error_states(config, SH):\n",
    "                yield e_config, m \n",
    "            config = parser.next_config(config, SH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the oracle with error state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset('en_ewt-ud-train-projectivized.conllu')\n",
    "dev_data = Dataset('en_ewt-ud-dev.conllu')\n",
    "example_sentence = list(train_data)[531]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 4, 2, 2]\n",
      "[0, 0, 0, 3, 3, 1, 3, 3, 0, 3, 3, 0, 3, 3, 1, 3, 3, 2, 3, 3, 0, 3, 3, 2, 3, 2, 3]\n",
      "[0, 0, 0, 1, 0, 0, 1, 2, 0, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "gold_heads = [h for w, t, h in example_sentence]\n",
    "gold_moves = [0, 0, 0, 1, 0, 0, 1, 2, 0, 2, 2]\n",
    "\n",
    "print(gold_heads)\n",
    "print(list(m for _, m in oracle_moves_with_error_state(gold_heads))) \n",
    "print(list(m for _, m in oracle_moves(gold_heads))) \n",
    "#assert list(m for _, m in oracle_moves(gold_heads)) == gold_moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual parser class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowParser(ArcStandardParser):\n",
    "\n",
    "    def __init__(self, vocab_words, vocab_tags, word_dim=50, tag_dim=10, hidden_dim=180, word_pretrained=None):\n",
    "        self.vocab_tags = vocab_tags\n",
    "        self.vocab_words = vocab_words\n",
    "        output_dim = 4 # SH, LA, RA, ES\n",
    "        # Embedding specs\n",
    "        embedding_specs = [(3, len(vocab_words), word_dim), (3, len(vocab_tags), tag_dim)]\n",
    "        # Initialize model\n",
    "        if word_pretrained != None:\n",
    "            self.model = FixedWindowModel(embedding_specs, hidden_dim, output_dim, word_pretrained=word_pretrained)\n",
    "        else:\n",
    "            self.model = FixedWindowModel(embedding_specs, hidden_dim, output_dim)\n",
    "\n",
    "    def featurize(self, words, tags, config):\n",
    "        # feature vector\n",
    "        features = torch.zeros(6, dtype=torch.int32) \n",
    "        # configuration\n",
    "        i, stack, heads = config\n",
    "        # 0. word form of the next word in the buffer\n",
    "        try:\n",
    "            features[0] = words[i]\n",
    "        except IndexError:\n",
    "            features[0] = self.vocab_words['<pad>'] \n",
    "        # 1. word form of the topmost word on the stack\n",
    "        try:\n",
    "            features[1] = words[stack[-1]]\n",
    "        except IndexError:\n",
    "            features[1] = self.vocab_words['<pad>']\n",
    "        # 2. word form of the second-topmost word on the stack\n",
    "        try:\n",
    "            features[2] = words[stack[-2]]\n",
    "        except IndexError:\n",
    "            features[2] = self.vocab_words['<pad>']\n",
    "        # 3. part-of-speech tag of the next word in the buffer\n",
    "        try:\n",
    "            features[3] = tags[i]\n",
    "        except IndexError:\n",
    "            features[3] = self.vocab_tags['<pad>'] \n",
    "        # 4. part-of-speech tag of the topmost word on the stack\n",
    "        try:\n",
    "            features[4] = tags[stack[-1]]\n",
    "        except IndexError:\n",
    "            features[4] = self.vocab_tags['<pad>']\n",
    "        # 5. part-of-speech tag of the second-topmost word on the stack\n",
    "        try:\n",
    "            features[5] = tags[stack[-2]]\n",
    "        except IndexError:\n",
    "            features[5] = self.vocab_tags['<pad>']\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def predict(self, words, tags):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        # Initialization\n",
    "        word_ids = []\n",
    "        tag_ids = []\n",
    "        # Encode words and tags into their ids\n",
    "        for w, t in zip(words, tags):\n",
    "            try:\n",
    "                word_ids.append(self.vocab_words[w])\n",
    "            except KeyError:\n",
    "                # word id for unknown words is '1'\n",
    "                word_ids.append(1)\n",
    "            try:\n",
    "                tag_ids.append(self.vocab_tags[t])\n",
    "            except KeyError:\n",
    "                # tag id for unknown tags is '1'\n",
    "                tag_ids.append(1)\n",
    "            \n",
    "        # Initial config\n",
    "        config = self.initial_config(len(words))\n",
    "        current_prob = 1\n",
    "        current_state = (config, current_prob)\n",
    "        # Initial priority queue\n",
    "        score_queue = queue.PriorityQueue() \n",
    "        # Softmax function\n",
    "        softmax_func = nn.Softmax(dim=1)\n",
    "        # Keep generating moves until final config\n",
    "        while not self.is_final_config(config):\n",
    "            # Get feature vector\n",
    "            features = self.featurize(word_ids, tag_ids, config)\n",
    "            features = features.reshape((1, 6))\n",
    "            # Feed through the network\n",
    "            output = self.model.forward(features)\n",
    "            # Calculate the probability of each moves with softmax function\n",
    "            probs = current_prob * softmax_func(output)\n",
    "            # Put 3 scores (3 outcomes of moves) into priority queue (valid ones)\n",
    "            # Note that the probability is negative bc of the minheap \n",
    "            for i in self.valid_moves(config):\n",
    "                pseudo_next_config = self.next_config(config, i)\n",
    "                score_queue.put((pseudo_next_config, -probs[0, i]))\n",
    "            # Choose the next state according to the priority queue\n",
    "            config, current_prob = score_queue.get()\n",
    "            current_prob = -current_prob\n",
    "        \n",
    "        predicted_heads = config[-1]\n",
    "            \n",
    "        \n",
    "        return predicted_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "word_vocab, tag_vocab = make_vocabs(train_data)\n",
    "my_parser = FixedWindowParser(word_vocab, tag_vocab, word_dim=50, tag_dim=10, hidden_dim=180, word_pretrained=None)\n",
    "my_parser.model\n",
    "# test prediction\n",
    "words = [w for w, t, h in example_sentence]\n",
    "tags = [t for w, t, h in example_sentence]\n",
    "print(my_parser.predict(words, tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generater that yields training examples for tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_examples_tagger(vocab_words, vocab_tags, gold_data, tagger, batch_size=100):\n",
    "    # Initialization\n",
    "    batch = torch.zeros((batch_size, 4), dtype=torch.int) \n",
    "    batch_labels = torch.zeros(batch_size, dtype=torch.long)\n",
    "    batch_index = 0\n",
    "    # Go through gold standard\n",
    "    for sentence in gold_data: \n",
    "        words = [w for w, t, _ in sentence]\n",
    "        tags = [t for w, t, _ in sentence]\n",
    "        # Go through sentence\n",
    "        for i, (word, tag, _) in enumerate(sentence):\n",
    "            # first word do not has pred_tags\n",
    "            if i == 0:\n",
    "                pred_tags = [0]\n",
    "            # encode to ids\n",
    "            word_ids = [tagger.vocab_words[w] for w in words]\n",
    "            # generate feature vector\n",
    "            features = tagger.featurize(word_ids, i , pred_tags)\n",
    "            # use gold label for pred_tags\n",
    "            pred_tags = [tagger.vocab_tags[tag]]\n",
    "            # put it in batch\n",
    "            batch[batch_index] = features\n",
    "            batch_labels[batch_index] = tagger.vocab_tags[tag]\n",
    "            batch_index += 1\n",
    "            # yield batch\n",
    "            if batch_index == batch_size:\n",
    "                batch_index = 0\n",
    "                yield batch, batch_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training loop for tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fixed_window_tagger(train_data, n_epochs=2, batch_size=100, lr=1e-3, word_pretrained=None):\n",
    "    # TODO: Replace the next line with your own code\n",
    "    \n",
    "    # Generate vocab for words and tags\n",
    "    vocab_words, vocab_tags = make_vocabs(train_data)\n",
    "    output_dim = len(vocab_tags)\n",
    "    \n",
    "    # Initialize the tagger\n",
    "    if word_pretrained != None:\n",
    "        tagger = FixedWindowTagger(vocab_words, vocab_tags, output_dim, word_pretrained=word_pretrained)\n",
    "    else:\n",
    "        tagger = FixedWindowTagger(vocab_words, vocab_tags, output_dim)\n",
    "    \n",
    "    # Initialize the optimizer. Here we use Adam rather than plain SGD\n",
    "    optimizer = optim.Adam(tagger.model.parameters(), lr=lr)\n",
    "    print(\"Initialization done\")\n",
    "    \n",
    "    # Training loop\n",
    "    for e in range(n_epochs):\n",
    "        start = time()\n",
    "        # batch\n",
    "        for X, y in training_examples_tagger(vocab_words, vocab_tags, train_data, tagger, batch_size):\n",
    "            # Reset the accumulated gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = tagger.model.forward(X)\n",
    "\n",
    "            # Loss\n",
    "            loss = F.cross_entropy(output, y)\n",
    "            \n",
    "            # Backward pass; propagates the loss and computes the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters of the model\n",
    "            optimizer.step()\n",
    "            #print(f'Per batch loss: {loss}')\n",
    "        print(f'Epoch {e} loss (train): {loss}')\n",
    "        end = time()\n",
    "        print(f'time for an epoch {end - start} s')\n",
    "    \n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from universal dependenciese project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset('en_ewt-ud-train-projectivized.conllu')\n",
    "dev_data = Dataset('en_ewt-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize tagger model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization done\n",
      "Epoch 0 loss (train): 0.2434326708316803\n",
      "time for an epoch 21.01133894920349 s\n",
      "Epoch 1 loss (train): 0.12937653064727783\n",
      "time for an epoch 23.964235067367554 s\n",
      "Epoch 2 loss (train): 0.09264914691448212\n",
      "time for an epoch 24.681602001190186 s\n",
      "Epoch 3 loss (train): 0.07722027599811554\n",
      "time for an epoch 25.22303295135498 s\n",
      "Epoch 4 loss (train): 0.05656846612691879\n",
      "time for an epoch 24.39499306678772 s\n",
      "Calculating accuracy ...\n",
      "Result: 0.9146\n"
     ]
    }
   ],
   "source": [
    "tagger = train_fixed_window_tagger(train_data, n_epochs=5)\n",
    "print('Calculating accuracy ...')\n",
    "print('Result: {:.4f}'.format(tag_accuracy(tagger, dev_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce a new 'retagged version' of data which use our tagging result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_ewt-ud-train-projectivized-retagged.conllu', 'wt', encoding=\"utf-8\") as target:\n",
    "    for sentence in Dataset_conllu('en_ewt-ud-train-projectivized.conllu'):\n",
    "        words = [columns[1] for columns in sentence]\n",
    "        tags = []\n",
    "        #print(sentence)\n",
    "        for i, t in enumerate(tagger.predict(words)):\n",
    "            sentence[i][3] = t\n",
    "        for columns in sentence:\n",
    "            print('\\t'.join(c for c in columns), file=target)\n",
    "        print(file=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_ewt-ud-dev-retagged.conllu', 'wt', encoding=\"utf-8\") as target:\n",
    "    for sentence in Dataset_conllu('en_ewt-ud-dev.conllu'):\n",
    "        words = [columns[1] for columns in sentence]\n",
    "        tags = []\n",
    "        #print(sentence)\n",
    "        for i, t in enumerate(tagger.predict(words)):\n",
    "            sentence[i][3] = t\n",
    "        for columns in sentence:\n",
    "            print('\\t'.join(c for c in columns), file=target)\n",
    "        print(file=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_examples_parser(vocab_words, vocab_tags, gold_data, parser, batch_size=100):\n",
    "    # Initializtion\n",
    "    batch = torch.zeros((batch_size, 6), dtype=torch.int32)\n",
    "    batch_labels = torch.zeros(batch_size, dtype=torch.long)\n",
    "    batch_index = 0\n",
    "    # Go through data\n",
    "    for sentence in gold_data:\n",
    "        # Extract data\n",
    "        words = [w for w, t, h in sentence]\n",
    "        tags = [t for w, t, h in sentence]\n",
    "        gold_heads = [h for w, t, h in sentence]\n",
    "        # Encode to ids\n",
    "        word_ids = []\n",
    "        tag_ids = []\n",
    "        for w, t in zip(words, tags):\n",
    "            try:\n",
    "                word_ids.append(parser.vocab_words[w])\n",
    "            except KeyError:\n",
    "                # word id for unknown words is '1'\n",
    "                word_ids.append(1)\n",
    "            try:\n",
    "                tag_ids.append(parser.vocab_tags[t])\n",
    "            except KeyError:\n",
    "                # tag id for unknown tags is '1'\n",
    "                tag_ids.append(1)\n",
    "        # Static oracle\n",
    "        for config, gold_move in oracle_moves_with_error_state(gold_heads): \n",
    "            # Feature vector\n",
    "            features = parser.featurize(word_ids, tag_ids, config)\n",
    "            # Put in batch\n",
    "            batch[batch_index] = features\n",
    "            batch_labels[batch_index] = gold_move\n",
    "            batch_index += 1\n",
    "            # Yielding batch\n",
    "            if batch_index == batch_size:\n",
    "                yield batch, batch_labels\n",
    "                batch_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fixed_window_parser(train_data, n_epochs=1, batch_size=100, lr=1e-2):\n",
    "    # TODO: Replace the next line with your own code\n",
    "    # Generate vocab for words and tags\n",
    "    vocab_words, vocab_tags = make_vocabs(train_data)\n",
    "    output_dim = len(vocab_tags)\n",
    "    \n",
    "    # Initialize the parser:\n",
    "    parser = FixedWindowParser(vocab_words, vocab_tags, word_dim=50, tag_dim=10, hidden_dim=180, word_pretrained=None)\n",
    "    \n",
    "    # Initialize the optimizer. Here we use Adam rather than plain SGD\n",
    "    optimizer = optim.Adam(parser.model.parameters(), lr=lr)\n",
    "    print(\"Initialization done\")\n",
    "    \n",
    "    # Training loop\n",
    "    for e in range(n_epochs):\n",
    "        start = time()\n",
    "        # batch\n",
    "        for X, y in training_examples_parser(vocab_words, vocab_tags, train_data, parser, batch_size):\n",
    "            # Reset the accumulated gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = parser.model.forward(X)\n",
    "\n",
    "            # Loss\n",
    "            loss = F.cross_entropy(output, y)\n",
    "            \n",
    "            # Backward pass; propagates the loss and computes the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters of the model\n",
    "            optimizer.step()\n",
    "            #print(f'Per batch loss: {loss}')\n",
    "            \n",
    "        print(f'Epoch {e} loss (train): {loss}')\n",
    "        end = time()\n",
    "        print(f'time for an epoch {end - start} s')\n",
    "    \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unlabelled attachment score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uas(parser, gold_data):\n",
    "    # TODO: Replace the next line with your own code\n",
    "    # Initialization\n",
    "    token_num = 0\n",
    "    correct_num = 0\n",
    "    # go through gold data\n",
    "    for sentence in gold_data:\n",
    "        # extract data\n",
    "        words = [w for w, t, h in sentence]\n",
    "        tags = [t for w, t, h in sentence]\n",
    "        gold_heads = [h for w, t, h in sentence]\n",
    "        # predict \n",
    "        predicted_heads = parser.predict(words, tags)\n",
    "        # count correct tokens (skip the first which is 'root')\n",
    "        for pred, label in zip(predicted_heads[1:], gold_heads[1:]):\n",
    "            # total count\n",
    "            token_num += 1\n",
    "            # correct count\n",
    "            if pred == label:\n",
    "                correct_num += 1\n",
    "        \n",
    "    return correct_num / token_num "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validate the model with the data produced by *our own tagger*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset('en_ewt-ud-train-projectivized-retagged.conllu')\n",
    "dev_data = Dataset('en_ewt-ud-dev-retagged.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization done\n",
      "Epoch 0 loss (train): 0.5318340063095093\n",
      "time for an epoch 135.72299194335938 s\n",
      "Epoch 1 loss (train): 0.4778480529785156\n",
      "time for an epoch 153.62976789474487 s\n"
     ]
    }
   ],
   "source": [
    "parser = train_fixed_window_parser(train_data, n_epochs=2)\n",
    "print('{:.4f}'.format(uas(parser, dev_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
